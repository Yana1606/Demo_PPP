from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from catboost import CatBoostClassifier

models = {
    'LogisticRegression': LogisticRegression(max_iter=1000),
    'RandomForest': RandomForestClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(probability=True),
    'CatBoost': CatBoostClassifier(
        random_state=42,
        verbose=0,
        allow_writing_files=False
    )
}


def get_model_params(trial, model_name):
    if model_name == 'LogisticRegression':
        return {
            'C': trial.suggest_float('C', 0.01, 10.0, log=True),
            'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs'])
        }

    elif model_name == 'RandomForest':
        return {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 15),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
            'max_features': trial.suggest_categorical(
                'max_features', ['sqrt', 'log2', None]
            )
        }

    elif model_name == 'GradientBoosting':
        return {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'learning_rate': trial.suggest_float(
                'learning_rate', 0.01, 0.3, log=True
            ),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
        }

    elif model_name == 'SVM':
        return {
            'C': trial.suggest_float('C', 0.1, 10.0, log=True),
            'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear']),
            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])
        }

    elif model_name == 'CatBoost':
        return {
            'iterations': trial.suggest_int('iterations', 100, 500),
            'learning_rate': trial.suggest_float(
                'learning_rate', 0.01, 0.3, log=True
            ),
            'depth': trial.suggest_int('depth', 3, 10),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
            'border_count': trial.suggest_int('border_count', 32, 255),
            'bagging_temperature': trial.suggest_float(
                'bagging_temperature', 0, 1
            )
        }
    else:
        return {}



from sklearn.model_selection import KFold, cross_val_score
import optuna


best_models = {}
best_scores = {}

for model_name in models.keys():
    print(f"\nПодбор гиперпараметров для {model_name}")

    def objective(trial):
        params = get_model_params(trial, model_name)

        if model_name == 'CatBoost':
            model = CatBoostClassifier(
                random_state=42,
                verbose=0,
                allow_writing_files=False,
                **params
            )
        else:
            model = models[model_name].set_params(**params)

        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        scores = cross_val_score(
            model,
            X_train_pca_scaler,
            y_train,
            cv=kf,
            scoring='accuracy'
        )
        return scores.mean()

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=5, show_progress_bar=True)

    print(f"Лучшие параметры: {study.best_params}")
    print(f"Лучшая accuracy: {study.best_value:.4f}")

    if model_name == 'CatBoost':
        final_model = CatBoostClassifier(
            random_state=42,
            verbose=0,
            allow_writing_files=False,
            **study.best_params
        )
    else:
        final_model = models[model_name].set_params(**study.best_params)

    final_model.fit(X_train_pca_scaler, y_train)

    best_models[model_name] = final_model
    best_scores[model_name] = study.best_value


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

results = []

for model_name, model in best_models.items():
    y_train_pred = model.predict(X_train_pca_scaler)
    y_test_pred = model.predict(X_test_pca_scaler)

    results.append({
        'Model': model_name,
        'Dataset': 'Train',
        'Accuracy': accuracy_score(y_train, y_train_pred),
        'Precision': precision_score(y_train, y_train_pred, average='weighted'),
        'Recall': recall_score(y_train, y_train_pred, average='weighted'),
        'F1': f1_score(y_train, y_train_pred, average='weighted')
    })

    results.append({
        'Model': model_name,
        'Dataset': 'Test',
        'Accuracy': accuracy_score(y_test, y_test_pred),
        'Precision': precision_score(y_test, y_test_pred, average='weighted'),
        'Recall': recall_score(y_test, y_test_pred, average='weighted'),
        'F1': f1_score(y_test, y_test_pred, average='weighted')
    })

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))



test_results = results_df[results_df['Dataset'] == 'Test']
best_model_row = test_results.loc[test_results['Accuracy'].idxmax()]

print(f"Лучшая модель: {best_model_row['Model']}")
print(f"Accuracy на тесте: {best_model_row['Accuracy']:.4f}")
print(f"F1 на тесте: {best_model_row['F1']:.4f}")
